{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a85c18",
   "metadata": {},
   "source": [
    "# Office Item Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fff734f",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606fe602",
   "metadata": {},
   "source": [
    "The **Office Item Classifier** project aims to develop an image classification model capable of distinguishing between 10 common office items. The 10 items are eraser, glue sticks, highlighter, mug, paper clips, pencil  pens, pencils, staplers, tape and USB sticks.\n",
    "\n",
    "The project uses a balanced dataset of 37,950 images collected from multiple sources including Kaggle, Roboflow Universe, and manually captured images. The dataset includes significant variations in lighting, angle, and background, with both static and dynamic data augmentations applied to enhance model robustness.\n",
    "\n",
    "The objective is to **train and evaluate models** to determine the most accurate and efficient approach for classifying office items.\n",
    "This notebook documents the entire process with explanations provided at each stage to justify design decisions and reflect on results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d1eb68",
   "metadata": {},
   "source": [
    "## 2. First Attempt - Custom CNN (train v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2a5d8b",
   "metadata": {},
   "source": [
    "The first attempt consist of building a custom CNN from scratch using TensorFlow/Keras.\n",
    "\n",
    "### Reasoning\n",
    "\n",
    "- Wanted to understand how the performance of a simple CNN would be before using a pre-trained model.\n",
    "- CNNs are widely used for image classification and can be customised easily\n",
    "- Used of 3 convolutional blocks and two dense layers with ReLy and softmax activations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e449781f",
   "metadata": {},
   "source": [
    "```python\n",
    "# Import dependencies.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the paths to datasets/\n",
    "TRAIN_DIR = \"data/train\"\n",
    "VAL_DIR = \"data/val\"\n",
    "TEST_DIR = \"data/test\"\n",
    "\n",
    "# Define image and training parameters.\n",
    "IMG_HEIGHT = 224 # Input image height.\n",
    "IMG_WIDTH = 224 # Input image width.\n",
    "BATCH_SIZE = 16 # Number of images per batch.\n",
    "EPOCHS = 20 # Maximum number of training epochs.\n",
    "NUM_CLASSES = 10 # Number of categories in the dataset.\n",
    "\n",
    "# Date augmentation and preprocessing.\n",
    "# Rescale pixel values to [0,1].\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load dataset using flow_flow_directiory.\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH), # Resize image to same size.\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical', # For multi-class classfication.\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH), # Resize image to same size.\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical', # For multi-class classfication.\n",
    "    shuffle=False # Shuffle the data for better training.\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH), # Resize image to same size.\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical', # For multi-class classfication.\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "# Build the CNN model.\n",
    "model = models.Sequential()\n",
    "\n",
    "# 1st Convolutional Block.\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2))) # Downsameple feature maps by a factor of 2.\n",
    "\n",
    "# 2nd Convolutional Block.\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# 3rd convolution block.\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten the 3D feature maps to 1D vector.\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Fully connect layer for learning non-linear combinations of features.\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "# Output the layer with softmax for multi class classification.\n",
    "model.add(layers.Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model with adam optimiser as it is adaptive.\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary() # Print model architecture.\n",
    "\n",
    "\n",
    "# Callback for better training.\n",
    "# Save the best model based on the validation accuracy.\n",
    "checkpoint = ModelCheckpoint(\"best_model.h5\", monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "# Stop training early if validation loss doesn't improve for 5 epochs.\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "\n",
    "callbacks = [checkpoint, earlystop]\n",
    "\n",
    "\n",
    "# Model.fit trains the model using batches from train_generator and validates on val_generator.\n",
    "# Epoch = one pass through the full training data.\n",
    "# Batches are shuffled to prevent the model from learning order instead of features.\n",
    "# EarlyStopping prevents overfitting by stopping if validation loss plateaus\n",
    "# ModelCheckpoint ensures you keep the best performing model.\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_generator.reset()  # Important to reset generator before predicting.\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"\\nTest Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# Generate predictions.\n",
    "y_pred = model.predict(test_generator)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1) # Convert probailities to class indices.\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Genereate confustion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Classification metrrics.\n",
    "f1 = f1_score(y_true, y_pred_classes, average='macro')\n",
    "print(f\"Macro F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(classification_report(y_true, y_pred_classes, target_names=test_generator.class_indices.keys()))\n",
    "\n",
    "# Plot trainig history.\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb068bab",
   "metadata": {},
   "source": [
    "### Result\n",
    "- The training accuracy was high, but the validation accuracy stayed low. This might have indicated the model was overfitted.\n",
    "- The model struggle to generalise.\n",
    "- This result motivated the team to move to YOLOv8 classifcation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258055fa",
   "metadata": {},
   "source": [
    "## 3. YOLOv8 Classification - train_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f89a15",
   "metadata": {},
   "source": [
    "After evaluating the custom CNN, we decided to use **YOLOv8 classification (YOLOv8-cls)**.  \n",
    "YOLOv8 leverages transfer learning from pre-trained weights, which improves accuracy and reduces training time compared to training a CNN from scratch.  \n",
    "\n",
    "### Reason:\n",
    "- Benefit from a modern architecture optimised for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d619e56f",
   "metadata": {},
   "source": [
    "``` python\n",
    "# Import dependencies.\n",
    "from ultralytics import YOLO # Import YOLO class from ultralytics library for object detection and classification.\n",
    "import torch # PyTorch library for tensor operations and GPU acceleration.\n",
    "from pathlib import Path # Pathlib for OS-independent file paths.\n",
    "import yaml # YAML library to read/write YAML configuration files.\n",
    "\n",
    "\n",
    "# Path to the root of the dataset folder containing train/val/test subfolders.\n",
    "DATASET_ROOT = Path(\"data\")  \n",
    "PROJECT_NAME = \"office_supplies_classifier\" # Name of the project; will be used to save outputs.\n",
    "MODEL_SIZE = \"x\" # Set model size.\n",
    "\n",
    "\n",
    "# List of class names for the classification task.\n",
    "CLASSES = [\n",
    "    \"erasers\", \"glue_sticks\", \"highlighters\", \"mugs\", \"paper_clips\",\n",
    "    \"pencils\", \"pens\", \"staplers\", \"tapes\", \"usb_sticks\"\n",
    "]\n",
    "\n",
    "\n",
    "# Training hyperparameters.\n",
    "EPOCHS = 50 # Total number of training epochs.\n",
    "BATCH_SIZE = 32 # Number of images per training batch.\n",
    "IMG_SIZE = 224 # Input image size.\n",
    "PATIENCE = 10 # Early stopping patience; stops training if val loss doesn't improve for 10 epochs.\n",
    "LEARNING_RATE = 0.001 # Initial learning rate.\n",
    "\n",
    "\n",
    "# Function to create the data.yaml configuration file required by YOLOv8.\n",
    "def create_data_yaml():\n",
    "    # Dictionary structure needed by YOLOv8.\n",
    "    data_config = {\n",
    "        'path': str(DATASET_ROOT.absolute()), # Absolute path to dataset root.\n",
    "        'train': 'train', # Folder containing training images.\n",
    "        'val': 'val', # Folder containing validation images.\n",
    "        'test': 'test', # Folder containing test images.\n",
    "        'names': {i: name for i, name in enumerate(CLASSES)} # Map numeric label → class name.\n",
    "    }\n",
    "    \n",
    "    yaml_path = DATASET_ROOT / 'data.yaml' # Path where YAML will be saved.\n",
    "    with open(yaml_path, 'w') as f: # Open file for writing.\n",
    "        yaml.dump(data_config, f, sort_keys=False) # Write YAML content without sorting keys.\n",
    "    \n",
    "    print(f\"Created data.yaml at {yaml_path}\") # Notify user.\n",
    "    return yaml_path # Return path to YAML file.\n",
    "\n",
    "\n",
    "# Function to train the YOLOv8 classification model.\n",
    "def train_model():\n",
    "    \n",
    "    # Create YAML config for dataset.\n",
    "    yaml_path = create_data_yaml()\n",
    "    \n",
    "    # Select device: use GPU if available, otherwise CPU.\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Load pre-trained YOLOv8 classification model of specified size.\n",
    "    model = YOLO(f'yolov8{MODEL_SIZE}-cls')\n",
    "    \n",
    "    # Train the model.\n",
    "    results = model.train(\n",
    "        data=str(DATASET_ROOT), # Path to dataset.\n",
    "        epochs=EPOCHS, # Number of training epochs.\n",
    "        batch=BATCH_SIZE, # Batch size.\n",
    "        imgsz=IMG_SIZE, # Resize all images to this size.\n",
    "        patience=PATIENCE, # Early stopping patience.\n",
    "        save=True, # Save trained model checkpoints.\n",
    "        device=device, # Training device.\n",
    "        project=PROJECT_NAME, # Project folder to save results.\n",
    "        name='train_v2', # Name of this training run.\n",
    "        exist_ok=True, # Overwrite existing project folder if it exists.\n",
    "        pretrained=True, # Use pretrained weights for transfer learning.\n",
    "        optimizer='AdamW', # Optimiser used (AdamW = Adam with weight decay).\n",
    "        lr0=LEARNING_RATE, # Initial learning rate.\n",
    "        lrf=0.01, # Final learning rate as fraction of initial LR (cosine annealing).\n",
    "        momentum=0.937, # Momentum for optimiser.\n",
    "        weight_decay=0.0005, # L2 weight decay for regularisation.\n",
    "        warmup_epochs=3, # Number of warmup epochs to gradually increase LR.\n",
    "        warmup_momentum=0.8, # Initial momentum during warmup.\n",
    "        cos_lr=True, # Use cosine annealing for learning rate schedule.\n",
    "        verbose=True, # Print detailed training logs.\n",
    "        cache=True, # Cache images in RAM for faster training.\n",
    "        amp=True, # Automatic Mixed Precision for faster training on GPU.\n",
    "        \n",
    "        # Data augmentation parameters.\n",
    "        hsv_h=0.015, # Hue adjustment factor.\n",
    "        hsv_s=0.5, # Saturation adjustment factor.\n",
    "        hsv_v=0.3, # Brightness adjustment factor.\n",
    "        degrees=10.0, # Random rotation in degrees.\n",
    "        translate=0.1, # Random translation.\n",
    "        scale=0.3, # Random scaling factor.\n",
    "        shear=0.0, # Shear angle.\n",
    "        perspective=0.0, # Perspective transform.\n",
    "        flipud=0.0, # Probability of flipping image vertically.\n",
    "        fliplr=0.5, # Probability of flipping image horizontally.\n",
    "        mosaic=0.0, # Mosaic augmentation probability.\n",
    "        mixup=0.0, # Mixup augmentation probability.\n",
    "        copy_paste=0.0 # Copy-paste augmentation probability.\n",
    "    )\n",
    "    \n",
    "    return model # Return trained model.\n",
    "\n",
    "# Entry point of the script.\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model = train_model() # Run training and store the trained model.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9c2ed9",
   "metadata": {},
   "source": [
    "### Result:\n",
    "- There was signs of overfitting.\n",
    "- The model was able to correctly recognise **some items some of the time**, especially distinctive objects like staplers and mugs, but struggled with visually similar items like pens vs pencils or glue sticks vs highlighters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c411ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bab0fce6",
   "metadata": {},
   "source": [
    "## 4. YOLOv8 Classification - train_v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9681e18d",
   "metadata": {},
   "source": [
    "### Key Changes in train_v3:\n",
    "- Epochs increased from 50 to 100 to give model more time to learn while using dropout and augmentations to mitigate overfitting.\n",
    "- Learning rate reduced from 0.001 t0 0.0005 to stabilise training.\n",
    "- Patience reduced from 10 to 5 for faster early stopping.\n",
    "- Dropout added (0.3) to reduce overfitting on similar-looking classes.\n",
    "- Data augmentation parameters strengthened to increase model robustness.\n",
    "- Mixup (0.15) added to further improve generalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e707d3b",
   "metadata": {},
   "source": [
    "``` python\n",
    "# Import dependencies.\n",
    "from ultralytics import YOLO # YOLO class for classification.\n",
    "import torch # PyTorch for tensor operations and GPU support.\n",
    "from pathlib import Path # OS-independent path handling.\n",
    "import yaml # For reading/writing YAML configuration files.\n",
    "\n",
    "\n",
    "# Path to dataset root and project setup.\n",
    "DATASET_ROOT = Path(\"data\")  \n",
    "PROJECT_NAME = \"office_supplies_classifier\" # Project folder for training outputs.\n",
    "MODEL_SIZE = \"m\" # Changed from 'x' → 'm' to reduce memory usage and speed up training.\n",
    "\n",
    "\n",
    "# List of class names for classification.\n",
    "CLASSES = [\n",
    "    \"erasers\", \"glue_sticks\", \"highlighters\", \"mugs\", \"paper_clips\",\n",
    "    \"pencils\", \"pens\", \"staplers\", \"tapes\", \"usb_sticks\"\n",
    "]\n",
    "\n",
    "\n",
    "# Training hyperparameters (modified based on v2 results).\n",
    "EPOCHS = 100 # Increased from 50 → 100 because the previous model showed overfitting early; more epochs allow better learning with proper regularization.\n",
    "BATCH_SIZE = 32 # Same as v2, balances GPU memory and stability.\n",
    "IMG_SIZE = 224 # Same input size for consistency.\n",
    "PATIENCE = 5 # Reduced from 10 → 5; early stopping triggers faster to prevent overfitting.\n",
    "LEARNING_RATE = 0.0005 # Lowered from 0.001 → 0.0005 to stabilize training on smaller batch size and prevent large weight updates.\n",
    "DROPOUT = 0.3 # Added dropout to combat overfitting, especially for visually similar classes.\n",
    "\n",
    "\n",
    "# Function to create YOLOv8 data.yaml file.\n",
    "def create_data_yaml():\n",
    "    data_config = {\n",
    "        'path': str(DATASET_ROOT.absolute()),\n",
    "        'train': 'train',\n",
    "        'val': 'val',\n",
    "        'test': 'test',\n",
    "        'names': {i: name for i, name in enumerate(CLASSES)}\n",
    "    }\n",
    "    \n",
    "    yaml_path = DATASET_ROOT / 'data.yaml'\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.dump(data_config, f, sort_keys=False)\n",
    "    \n",
    "    print(f\"✓ Created data.yaml at {yaml_path}\")\n",
    "    return yaml_path\n",
    "\n",
    "\n",
    "# Function to train YOLOv8 classification model.\n",
    "def train_model():\n",
    "    \n",
    "    # Create dataset YAML configuration.\n",
    "    yaml_path = create_data_yaml()\n",
    "    \n",
    "    # Select device: GPU if available, else CPU.\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Load pre-trained YOLOv8 classification model of chosen size.\n",
    "    model = YOLO(f'yolov8{MODEL_SIZE}-cls')\n",
    "    \n",
    "    # Train model with updated hyperparameters and augmentations.\n",
    "    results = model.train(\n",
    "        data=str(DATASET_ROOT), # Dataset path.\n",
    "        epochs=EPOCHS, # Increased epochs for more learning capacity.\n",
    "        batch=BATCH_SIZE,\n",
    "        imgsz=IMG_SIZE,\n",
    "        patience=PATIENCE, # Early stopping patience reduced to 5.\n",
    "        save=True,\n",
    "        device=device,\n",
    "        workers=8, # Set number of data loading threads for speed.\n",
    "        project=PROJECT_NAME,\n",
    "        name='train_v3', # Name of this run.\n",
    "        exist_ok=True,\n",
    "        pretrained=True,\n",
    "        optimizer='AdamW',\n",
    "        \n",
    "        lr0=LEARNING_RATE, # Initial LR lowered to 0.0005 for stability.\n",
    "        lrf=0.001, # Final LR fraction adjusted for slower decay.\n",
    "        momentum=0.937,\n",
    "        weight_decay=0.001, # Slightly increased from 0.0005 → 0.001 for stronger regularization.\n",
    "        \n",
    "        warmup_epochs=5, # Warmup extended to avoid sudden gradient spikes.\n",
    "        warmup_momentum=0.8,\n",
    "        cos_lr=True,\n",
    "        verbose=True,\n",
    "        \n",
    "        cache=False, # Disabled caching to save RAM; could be enabled if dataset is small.\n",
    "        \n",
    "        amp=True, # Mixed precision for faster GPU training.\n",
    "        \n",
    "        # Data augmentation (increased compared to v2 for robustness)\n",
    "        hsv_h=0.03, # Slightly stronger hue shift.\n",
    "        hsv_s=0.7, # Stronger saturation augmentation.\n",
    "        hsv_v=0.4, # Stronger brightness variation.\n",
    "        degrees=25.0, # More rotation augmentation.\n",
    "        translate=0.2,\n",
    "        scale=0.5,\n",
    "        shear=5.0,\n",
    "        perspective=0.0005,\n",
    "        flipud=0.0,\n",
    "        fliplr=0.5,\n",
    "        \n",
    "        mosaic=0.0,\n",
    "        mixup=0.15, # Mixup added to reduce overfitting.\n",
    "        copy_paste=0.0,\n",
    "        \n",
    "        dropout=DROPOUT, # Dropout added to reduce overfitting.\n",
    "        \n",
    "        label_smoothing=0.1 # Helps model generalise better on visually similar classes.\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Entry point.\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model = train_model() # Train the model and store it.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2608c44",
   "metadata": {},
   "source": [
    "### Result (train_v3):\n",
    "\n",
    "- Training accuracy improved slightly compared to v2, reaching around **92–94%**, while validation accuracy also increased to about **85–87%**.  \n",
    "- The model was now able to **detect objects more reliably**, especially distinctive items like staplers, mugs, and USB sticks.  \n",
    "- Visually similar items, such as pens vs pencils or glue sticks vs highlighters, were still misclassified most of the time, indicating that the model had not fully learned the subtle features.  \n",
    "- Confusion matrix showed reduced misclassifications, but errors still remained between similar-looking classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da83999e",
   "metadata": {},
   "source": [
    "## 5. YOLOv8 Classification - train_v4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ccadb2",
   "metadata": {},
   "source": [
    "###  Key Changes in train_v4:\n",
    "-Model size increased from m to l to improve accuracy on more complex features.\n",
    "-Batch size reduced from 32 to 16 to accommodate larger model and avoid GPU memory issues.\n",
    "-Input image size increased from 224 to 320 for higher resolution feature extraction.\n",
    "-Patience increased from 5 to 15 to allow more epochs for early stopping due to larger model.\n",
    "-Learning rate reduced from 0.0005 to 0.0001 for more stable training with large model.\n",
    "-Dropout reduced from 0.3 to 0.2 to balance regularisation and training capacity.\n",
    "-Number of data loading workers reduced from 8 to 4, matching smaller batch size and avoiding potential bottlenecks.\n",
    "-Optimiser parameters adjusted: momentum lowered from 0.937 to 0.9, weight_decay reduced from 0.001 to 0.0005.\n",
    "\n",
    "#### Augmentation changes:\n",
    "-HSV, rotation, translate, scale, shear, perspective, fliplr slightly reduced for more conservative augmentations.\n",
    "-Added auto_augment='randaugment' for improved generalisation.\n",
    "-crop_fraction added to control crop augmentation.\n",
    "-Mixup reduced from 0.15 to 0.1 due to stronger augmentations.\n",
    "-Label smoothing reduced from 0.1 to 0.05 to allow model to learn larger dataset more confidently.\n",
    "-Added save_period=10 to periodically save checkpoints.\n",
    "-Added warmup_bias_lr=0.01 for better bias initialisation.\n",
    "-Added close_mosaic=10 for YOLO-specific training optimisation.\n",
    "-Added plots=True and val=True to enable validation and visualisations during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032720a4",
   "metadata": {},
   "source": [
    "``` python\n",
    "# Import dependencies.\n",
    "from ultralytics import YOLO # YOLO class for classification.\n",
    "import torch  # PyTorch for tensor operations and GPU support.\n",
    "from pathlib import Path  # OS-independent path handling.\n",
    "import yaml  # For reading/writing YAML configuration files.\n",
    "\n",
    "\n",
    "# Path to dataset root and project setup.\n",
    "DATASET_ROOT = Path(\"data\")  \n",
    "PROJECT_NAME = \"office_supplies_classifier\" # Project folder for training outputs.\n",
    "MODEL_SIZE = \"l\" # Changed from 'm' to 'l' to improve accuracy on more complex features.\n",
    "\n",
    "\n",
    "# List of class names for classification.\n",
    "CLASSES = [\n",
    "    \"erasers\", \"glue_sticks\", \"highlighters\", \"mugs\", \"paper_clips\",\n",
    "    \"pencils\", \"pens\", \"staplers\", \"tapes\", \"usb_sticks\"\n",
    "]\n",
    "\n",
    "# Training hyperparameters (modified based on v3 results).\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 16 # Reduced from 32 to 16 to handle larger model on GPU.\n",
    "IMG_SIZE = 320 # Increased from 224 to 320 for higher resolution input.\n",
    "PATIENCE = 15 # Increased from 5 to 15 to allow more epochs for early stopping.\n",
    "LEARNING_RATE = 0.0001 # Reduced from 0.0005 to 0.0001 for stability on larger model.\n",
    "DROPOUT = 0.2 # Reduced from 0.3 to 0.2 to balance regularisation and learning capacity.\n",
    "\n",
    "\n",
    "# Function to create YOLOv8 data.yaml file.\n",
    "def create_data_yaml():\n",
    "    data_config = {\n",
    "        'path': str(DATASET_ROOT.absolute()),\n",
    "        'train': 'train',\n",
    "        'val': 'val',\n",
    "        'test': 'test',\n",
    "        'names': {i: name for i, name in enumerate(CLASSES)}\n",
    "    }\n",
    "    \n",
    "    yaml_path = DATASET_ROOT / 'data.yaml'\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.dump(data_config, f, sort_keys=False)\n",
    "    \n",
    "    print(f\"✓ Created data.yaml at {yaml_path}\")\n",
    "    return yaml_path\n",
    "\n",
    "\n",
    "# Function to train YOLOv8 classification model.\n",
    "def train_model():\n",
    "    \n",
    "    # Create dataset YAML configuration.\n",
    "    yaml_path = create_data_yaml()\n",
    "    \n",
    "    # Select device: GPU if available, else CPU.\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Load pre-trained YOLOv8 classification model of chosen size.\n",
    "    model = YOLO(f'yolov8{MODEL_SIZE}-cls')\n",
    "    \n",
    "    # Train model with updated hyperparameters and augmentations.\n",
    "    results = model.train(\n",
    "        data=str(DATASET_ROOT),\n",
    "        epochs=EPOCHS,\n",
    "        batch=BATCH_SIZE,\n",
    "        imgsz=IMG_SIZE,\n",
    "        patience=PATIENCE,\n",
    "        save=True,\n",
    "        save_period=10, # Added periodic checkpoint saving.\n",
    "        device=device,\n",
    "        workers=4, # Reduced from 8 to 4 for smaller batch.\n",
    "        project=PROJECT_NAME,\n",
    "        name='train_v4',\n",
    "        exist_ok=True,\n",
    "        pretrained=True,\n",
    "        optimizer='AdamW',\n",
    "        \n",
    "        lr0=LEARNING_RATE,\n",
    "        lrf=0.0001, # Adjusted to smaller final LR fraction.\n",
    "        momentum=0.9, # Reduced from 0.937 to 0.9\n",
    "        weight_decay=0.0005, # Reduced from 0.001 to 0.0005\n",
    "        \n",
    "        warmup_epochs=5,\n",
    "        warmup_momentum=0.8,\n",
    "        warmup_bias_lr=0.01, # Added for better bias initialisation.\n",
    "        cos_lr=True,\n",
    "        \n",
    "        verbose=True,\n",
    "        cache=False,\n",
    "        amp=True,\n",
    "        \n",
    "        hsv_h=0.02, # Slightly reduced from 0.03\n",
    "        hsv_s=0.5, # Reduced from 0.7\n",
    "        hsv_v=0.3, # Reduced from 0.4\n",
    "        degrees=15.0, # Reduced from 25\n",
    "        translate=0.15, # Reduced from 0.2\n",
    "        scale=0.4, # Reduced from 0.5\n",
    "        shear=2.0, # Reduced from 5\n",
    "        perspective=0.0002, # Reduced from 0.0005\n",
    "        flipud=0.0,\n",
    "        fliplr=0.3, # Reduced from 0.5\n",
    "        \n",
    "        mosaic=0.0,\n",
    "        mixup=0.1, # Reduced from 0.15 due to stronger augmentations.\n",
    "        copy_paste=0.0,\n",
    "        auto_augment='randaugment', # Added for stronger generalisation.\n",
    "        erasing=0.0,\n",
    "        crop_fraction=1.0, # Added for crop control.\n",
    "        \n",
    "        dropout=DROPOUT,\n",
    "        label_smoothing=0.05, # Reduced from 0.1 to let larger model learn more confidently.\n",
    "        \n",
    "        close_mosaic=10, # Added YOLO-specific optimisation.\n",
    "        plots=True, # Added to visualise training progress.\n",
    "        val=True # Added to perform validation during training.\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "# Entry point.\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model = train_model() # Train the model and store it.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ecc4f",
   "metadata": {},
   "source": [
    "### Result (train_v4):\n",
    "- The model could now **detect a larger number of objects reliably**, especially medium-sized and distinctive items like staplers, mugs, and USB sticks.  \n",
    "- Misclassifications still occurred for very similar classes (pens vs pencils, glue sticks vs highlighters), but these errors were less frequent than in train_v3.  \n",
    "- Confusion matrices showed improved separation between visually similar items, indicating better feature learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f193b442",
   "metadata": {},
   "source": [
    "## 6. YOLOv8 Classification - train_v5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871079e5",
   "metadata": {},
   "source": [
    "### Key Changes in train_v5:\n",
    "- Epochs increased from 100 to 150 to allow more training on stronger augmentations.\n",
    "- Patience increased from 15 to 25 to allow longer early stopping window for extreme augmentations.\n",
    "- Learning rate reduced from 0.0001 to 0.00005 for more stable training with extreme augmentations.\n",
    "- Dropout increased from 0.2 to 0.3 to prevent overfitting under extreme augmentations.\n",
    "\n",
    "#### Augmentation changes:\n",
    "- HSV augmentation significantly increased: hsv_h 0.02 to 0.5, hsv_s 0.5 to 0.9, hsv_v 0.3 to 0.9.\n",
    "- Rotation (degrees) increased from 15 to 180 for extreme rotational augmentation.\n",
    "-Translation increased from 0.15 to 0.3.\n",
    "-Scale increased from 0.4 to 0.9.\n",
    "-Shear increased from 2 to 10.\n",
    "-Perspective increased from 0.0002 to 0.001.\n",
    "-Vertical flip (flipud) added 0 to 0.5.\n",
    "-Mixup increased from 0.1 to 0.3.\n",
    "-Erasing added from 0 to 0.4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35b1c44",
   "metadata": {},
   "source": [
    "``` python\n",
    "# Import dependencies.\n",
    "from ultralytics import YOLO # YOLO class for classification.\n",
    "import torch  # PyTorch for tensor operations and GPU support.\n",
    "from pathlib import Path  # OS-independent path handling.\n",
    "import yaml  # For reading/writing YAML configuration files.\n",
    "\n",
    "\n",
    "# Path to dataset root and project setup.\n",
    "DATASET_ROOT = Path(\"data\")  \n",
    "PROJECT_NAME = \"office_supplies_classifier\" # Project folder for training outputs.\n",
    "MODEL_SIZE = \"l\" # Changed from 'm' to 'l' to improve accuracy on more complex features.\n",
    "\n",
    "\n",
    "# List of class names for classification.\n",
    "CLASSES = [\n",
    "    \"erasers\", \"glue_sticks\", \"highlighters\", \"mugs\", \"paper_clips\",\n",
    "    \"pencils\", \"pens\", \"staplers\", \"tapes\", \"usb_sticks\"\n",
    "]\n",
    "\n",
    "# Training hyperparameters (modified based on v4 results).\n",
    "EPOCHS = 150 # Increased from 100 to 150 for longer training with extreme augmentations.\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 320\n",
    "PATIENCE = 25 # Increased from 15 to 25 to allow longer early stopping.\n",
    "LEARNING_RATE = 0.00005 # Reduced from 0.0001 to 0.00005 for more stable training.\n",
    "DROPOUT = 0.3 # Increased from 0.2 to 0.3 to prevent overfitting under extreme augmentations.\n",
    "\n",
    "\n",
    "# Function to create YOLOv8 data.yaml file.\n",
    "def create_data_yaml():\n",
    "    data_config = {\n",
    "        'path': str(DATASET_ROOT.absolute()),\n",
    "        'train': 'train',\n",
    "        'val': 'val',\n",
    "        'test': 'test',\n",
    "        'names': {i: name for i, name in enumerate(CLASSES)}\n",
    "    }\n",
    "    \n",
    "    yaml_path = DATASET_ROOT / 'data.yaml'\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.dump(data_config, f, sort_keys=False)\n",
    "    \n",
    "    print(f\"✓ Created data.yaml at {yaml_path}\")\n",
    "    return yaml_path\n",
    "\n",
    "\n",
    "# Function to train YOLOv8 classification model.\n",
    "def train_model():\n",
    "    \n",
    "    # Create dataset YAML configuration.\n",
    "    yaml_path = create_data_yaml()\n",
    "    \n",
    "    # Select device: GPU if available, else CPU.\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Load pre-trained YOLOv8 classification model of chosen size.\n",
    "    model = YOLO(f'yolov8{MODEL_SIZE}-cls')\n",
    "    \n",
    "    # Train model with updated hyperparameters and augmentations.\n",
    "    results = model.train(\n",
    "        data=str(DATASET_ROOT),\n",
    "        epochs=EPOCHS,\n",
    "        batch=BATCH_SIZE,\n",
    "        imgsz=IMG_SIZE,\n",
    "        patience=PATIENCE,\n",
    "        save=True,\n",
    "        save_period=10,\n",
    "        device=device,\n",
    "        workers=4,\n",
    "        project=PROJECT_NAME,\n",
    "        name='train_v5',\n",
    "        exist_ok=True,\n",
    "        pretrained=True,\n",
    "        optimizer='AdamW',\n",
    "        \n",
    "        lr0=LEARNING_RATE,\n",
    "        lrf=0.0001,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0005,\n",
    "        \n",
    "        warmup_epochs=5,\n",
    "        warmup_momentum=0.8,\n",
    "        warmup_bias_lr=0.01,\n",
    "        cos_lr=True,\n",
    "        \n",
    "        verbose=True,\n",
    "        cache=False,\n",
    "        amp=True,\n",
    "        \n",
    "        hsv_h=0.5, # Increased from 0.02 to 0.5 for extreme hue augmentation.\n",
    "        hsv_s=0.9, # Increased from 0.5 to 0.9\n",
    "        hsv_v=0.9, # Increased from 0.3 to 0.9\n",
    "        degrees=180.0, # Increased from 15 to 180 for extreme rotation.\n",
    "        translate=0.3, # Increased from 0.15 to 0.3\n",
    "        scale=0.9, # Increased from 0.4 to 0.9\n",
    "        shear=10.0, # Increased from 2 to 10\n",
    "        perspective=0.001, # Increased from 0.0002 to 0.001\n",
    "        flipud=0.5, # Added vertical flip\n",
    "        fliplr=0.5, # Increased from 0.3 to 0.5\n",
    "        \n",
    "        mosaic=0.0,\n",
    "        mixup=0.3, # Increased from 0.1 to 0.3\n",
    "        copy_paste=0.0,\n",
    "        auto_augment='randaugment',\n",
    "        erasing=0.4, # Added erasing for stronger augmentation.\n",
    "        \n",
    "        dropout=DROPOUT,\n",
    "        \n",
    "        close_mosaic=10,\n",
    "        plots=True,\n",
    "        val=True,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Entry point.\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model = train_model() # Train the model and store it.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b89c48",
   "metadata": {},
   "source": [
    "### Result (train_v5):\n",
    "\n",
    "- The model was able to **reliably recognise 7 out of 10 classes**, including distinctive items like staplers, mugs, USB sticks, erasers, glue sticks, tapes, and paper clips.  \n",
    "- The remaining **3 visually similar classes — pens, pencils, and highlighters — were misclassified**, indicating that the model still finds it difficult to capture subtle differences between these items.  \n",
    "- Confusion matrices confirmed that misclassifications were mostly limited to these similar classes, while the rest were almost perfectly recognised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce1770",
   "metadata": {},
   "source": [
    "## 7. YOLOv8 Classification - train_v6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a570155",
   "metadata": {},
   "source": [
    "### Key Changes in train_v6:\n",
    "- Input image size increased from 320  to 384 to allow better recognition of visually similar objects (pens, pencils, highlighters).\n",
    "- Patience increased from 25  to 30 to allow more epochs for early stopping with extreme augmentations.\n",
    "- Learning rate reduced from 0.00005  to 0.00003 for more stable training under extreme augmentations.\n",
    "- Dropout increased from 0.3  to 0.4 to prevent overfitting on high-resolution inputs and extreme augmentations.\n",
    "- Final learning rate fraction (lrf) reduced from 0.0001  to 0.00001 for slower decay with larger input.\n",
    "- Momentum increased from 0.9  to 0.937.\n",
    "- Weight decay increased from 0.0005  to 0.001 for stronger regularization.\n",
    "\n",
    "#### Augmentation changes:\n",
    "- HSV augmentation further increased: hsv_h 0.5  to 0.8, hsv_s 0.9  to 0.95, hsv_v 0.9  to 0.95.\n",
    "- Translation increased from 0.3  to 0.4.\n",
    "- Scale increased from 0.9  to 0.95.\n",
    "- Shear increased from 10  to 15.\n",
    "- Perspective increased from 0.001  to 0.002.\n",
    "- Mixup increased from 0.3  to 0.5.\n",
    "- Erasing increased from 0.4  to 0.6.\n",
    "- Crop fraction added from 1.0  to 0.5 for better extreme augmentation control.\n",
    "- Close mosaic increased from 10  to 15 for YOLO-specific optimization.\n",
    "- Label smoothing increased from 0.0  to 0.15 to improve generalization for visually similar classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba738c",
   "metadata": {},
   "source": [
    "```python\n",
    "# Import dependencies.\n",
    "from ultralytics import YOLO # YOLO class for classification.\n",
    "import torch  # PyTorch for tensor operations and GPU support.\n",
    "from pathlib import Path  # OS-independent path handling.\n",
    "import yaml  # For reading/writing YAML configuration files.\n",
    "\n",
    "\n",
    "# Path to dataset root and project setup.\n",
    "DATASET_ROOT = Path(\"data\")  \n",
    "PROJECT_NAME = \"office_supplies_classifier\" # Project folder for training outputs.\n",
    "MODEL_SIZE = \"l\" # Changed from 'm' to 'l' to improve accuracy on more complex features.\n",
    "\n",
    "\n",
    "# List of class names for classification.\n",
    "CLASSES = [\n",
    "    \"erasers\", \"glue_sticks\", \"highlighters\", \"mugs\", \"paper_clips\",\n",
    "    \"pencils\", \"pens\", \"staplers\", \"tapes\", \"usb_sticks\"\n",
    "]\n",
    "\n",
    "# Training hyperparameters (modified based on v3 results).\n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 384 # Increased from 320  to 384 to improve recognition of pens, pencils, and highlighters.\n",
    "PATIENCE = 30 # Increased from 25  to 30 to allow more early stopping epochs.\n",
    "LEARNING_RATE = 0.00003 # Reduced from 0.00005  to 0.00003 for stable training with extreme augmentations.\n",
    "DROPOUT = 0.4 # Increased from 0.3  to 0.4 to prevent overfitting with higher resolution and extreme augmentations.\n",
    "\n",
    "# Function to create YOLOv8 data.yaml file.\n",
    "def create_data_yaml():\n",
    "    data_config = {\n",
    "        'path': str(DATASET_ROOT.absolute()),\n",
    "        'train': 'train',\n",
    "        'val': 'val',\n",
    "        'test': 'test',\n",
    "        'names': {i: name for i, name in enumerate(CLASSES)}\n",
    "    }\n",
    "    \n",
    "    yaml_path = DATASET_ROOT / 'data.yaml'\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.dump(data_config, f, sort_keys=False)\n",
    "    \n",
    "    print(f\"✓ Created data.yaml at {yaml_path}\")\n",
    "    return yaml_path\n",
    "\n",
    "\n",
    "# Function to train YOLOv8 classification model.\n",
    "def train_model():\n",
    "    \n",
    "    # Create dataset YAML configuration.\n",
    "    yaml_path = create_data_yaml()\n",
    "    \n",
    "    # Select device: GPU if available, else CPU.\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Load pre-trained YOLOv8 classification model of chosen size.\n",
    "    model = YOLO(f'yolov8{MODEL_SIZE}-cls')\n",
    "    \n",
    "    # Train model with updated hyperparameters and augmentations.\n",
    "    results = model.train(\n",
    "        data=str(DATASET_ROOT),\n",
    "        epochs=EPOCHS,\n",
    "        batch=BATCH_SIZE,\n",
    "        imgsz=IMG_SIZE,\n",
    "        patience=PATIENCE,\n",
    "        save=True,\n",
    "        save_period=10,\n",
    "        device=device,\n",
    "        workers=4,\n",
    "        project=PROJECT_NAME,\n",
    "        name='train_v6',\n",
    "        exist_ok=True,\n",
    "        pretrained=True,\n",
    "        optimizer='AdamW',\n",
    "        \n",
    "        lr0=LEARNING_RATE,\n",
    "        lrf=0.00001, # Reduced from 0.0001 to 0.00001 for slower decay with larger input.\n",
    "        momentum=0.937, # Increased from 0.9 to 0.937\n",
    "        weight_decay=0.001, # Increased from 0.0005 to 0.001\n",
    "        \n",
    "        warmup_epochs=5,\n",
    "        warmup_momentum=0.8,\n",
    "        warmup_bias_lr=0.01,\n",
    "        cos_lr=True,\n",
    "        \n",
    "        verbose=True,\n",
    "        cache=False,\n",
    "        amp=True,\n",
    "        \n",
    "        hsv_h=0.8, # Increased from 0.5 to 0.8 for stronger hue augmentation.\n",
    "        hsv_s=0.95, # Increased from 0.9 to 0.95\n",
    "        hsv_v=0.95, # Increased from 0.9 to 0.95\n",
    "        \n",
    "        degrees=180.0,\n",
    "        translate=0.4, # Increased from 0.3 to 0.4\n",
    "        scale=0.95, # Increased from 0.9 to 0.95\n",
    "        shear=15.0, # Increased from 10 to 15\n",
    "        perspective=0.002, # Increased from 0.001 to 0.002\n",
    "        flipud=0.5,\n",
    "        fliplr=0.5,\n",
    "        \n",
    "        mosaic=0.0,\n",
    "        mixup=0.5, # Increased from 0.3 to 0.5\n",
    "        copy_paste=0.0,\n",
    "        auto_augment='randaugment',\n",
    "        erasing=0.6, # Increased from 0.4 to 0.6\n",
    "        crop_fraction=0.5, # Added from 1.0 to 0.5 for better augmentation control.\n",
    "        \n",
    "        dropout=DROPOUT,\n",
    "        label_smoothing=0.15, # Addedto improve generalisation for visually similar classes.\n",
    "        \n",
    "        close_mosaic=15, # Increased from 10 to 15 for YOLO-specific optimisation.\n",
    "        plots=True,\n",
    "        val=True,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Entry point.\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model = train_model() # Train the model and store it.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77911f1",
   "metadata": {},
   "source": [
    "### Result (train_v6):\n",
    "\n",
    "- The model was able to **successfully recognise all 10 classes**, including visually similar items like pens, pencils, and highlighters.  \n",
    "- Remaining challenges were related to **object orientation and cluttered backgrounds**: some images where objects were rotated unusually or partially occluded still caused minor misclassifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e8f102",
   "metadata": {},
   "source": [
    "## 8. YOLOv8 Classification - train_v7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3bef76",
   "metadata": {},
   "source": [
    "### Downlaod all neccessary libraies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "635c7dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics==8.3.218 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (8.3.218)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics==8.3.218) (2.1.2)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics==8.3.218) (3.9.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics==8.3.218) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics==8.3.218) (11.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics==8.3.218) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics==8.3.218) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics==8.3.218) (1.16.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics==8.3.218) (2.9.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics==8.3.218) (0.24.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\mehar\\appdata\\roaming\\python\\python312\\site-packages (from ultralytics==8.3.218) (6.1.0)\n",
      "Requirement already satisfied: polars in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics==8.3.218) (1.34.0)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics==8.3.218) (2.0.17)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics==8.3.218) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics==8.3.218) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics==8.3.218) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics==8.3.218) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics==8.3.218) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics==8.3.218) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mehar\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib>=3.3.0->ultralytics==8.3.218) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mehar\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.3.218) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.23.0->ultralytics==8.3.218) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.23.0->ultralytics==8.3.218) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.23.0->ultralytics==8.3.218) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.23.0->ultralytics==8.3.218) (2024.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics==8.3.218) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\mehar\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.8.0->ultralytics==8.3.218) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics==8.3.218) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics==8.3.218) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics==8.3.218) (3.1.3)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics==8.3.218) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics==8.3.218) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics==8.3.218) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics==8.3.218) (2.1.5)\n",
      "Requirement already satisfied: polars-runtime-32==1.34.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from polars->ultralytics==8.3.218) (1.34.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.9.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.9.0) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\mehar\\appdata\\roaming\\python\\python312\\site-packages (from torch==2.9.0) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.9.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.9.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.9.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.9.0) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.9.0) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch==2.9.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch==2.9.0) (2.1.5)\n",
      "Requirement already satisfied: PyYAML==6.0.2 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: scikit-learn==1.7.2 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.7.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn==1.7.2) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn==1.7.2) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn==1.7.2) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn==1.7.2) (3.6.0)\n",
      "Requirement already satisfied: numpy==2.1.2 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: matplotlib==3.9.2 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib==3.9.2) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib==3.9.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib==3.9.2) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib==3.9.2) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib==3.9.2) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib==3.9.2) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mehar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib==3.9.2) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mehar\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib==3.9.2) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mehar\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib==3.9.2) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install ultralytics==8.3.218\n",
    "!python -m pip install torch==2.9.0\n",
    "!python -m pip install PyYAML==6.0.2\n",
    "!python -m pip install scikit-learn==1.7.2\n",
    "!python -m pip install numpy==2.1.2 matplotlib==3.9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2ccd86",
   "metadata": {},
   "source": [
    "### Train 7th Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf88924b",
   "metadata": {},
   "source": [
    "#### Key Changes in train_v7:\n",
    "- Epochs reduced from 150 to 50 for faster training and experimentation.\n",
    "- Input image scale slightly reduced from 0.95 to 0.9 to balance extreme augmentation effects.\n",
    "- Rotation (degrees) increased from 180 to 270 for more aggressive rotational augmentation.\n",
    "- Erasing slightly reduced from 0.6 to 0.5.\n",
    "- Crop fraction slightly increased from 0.5 to 0.55 for better augmentation coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedbb6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies.\n",
    "from ultralytics import YOLO # YOLO class for classification.\n",
    "import torch  # PyTorch for tensor operations and GPU support.\n",
    "from pathlib import Path  # OS-independent path handling.\n",
    "import yaml  # For reading/writing YAML configuration files.\n",
    "\n",
    "\n",
    "# Path to dataset root and project setup.\n",
    "DATASET_ROOT = Path(\"data\")  \n",
    "PROJECT_NAME = \"office_supplies_classifier\" # Project folder for training outputs.\n",
    "MODEL_SIZE = \"l\" # Changed from 'm' to 'l' to improve accuracy on more complex features.\n",
    "\n",
    "\n",
    "# List of class names for classification.\n",
    "CLASSES = [\n",
    "    \"erasers\", \"glue_sticks\", \"highlighters\", \"mugs\", \"paper_clips\",\n",
    "    \"pencils\", \"pens\", \"staplers\", \"tapes\", \"usb_sticks\"\n",
    "]\n",
    "\n",
    "# Training hyperparameters (modified based on v3 results).\n",
    "EPOCHS = 50 # Reduced from 150 to 50 for faster training/experimentation.\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 384\n",
    "PATIENCE = 30\n",
    "LEARNING_RATE = 0.00003\n",
    "DROPOUT = 0.4\n",
    "\n",
    "\n",
    "# Function to create YOLOv8 data.yaml file.\n",
    "def create_data_yaml():\n",
    "    data_config = {\n",
    "        'path': str(DATASET_ROOT.absolute()),\n",
    "        'train': 'train',\n",
    "        'val': 'val',\n",
    "        'test': 'test',\n",
    "        'names': {i: name for i, name in enumerate(CLASSES)}\n",
    "    }\n",
    "    \n",
    "    yaml_path = DATASET_ROOT / 'data.yaml'\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.dump(data_config, f, sort_keys=False)\n",
    "    \n",
    "    print(f\"✓ Created data.yaml at {yaml_path}\")\n",
    "    return yaml_path\n",
    "\n",
    "\n",
    "# Function to train YOLOv8 classification model.\n",
    "def train_model():\n",
    "    \n",
    "    # Create dataset YAML configuration.\n",
    "    yaml_path = create_data_yaml()\n",
    "    \n",
    "    # Select device: GPU if available, else CPU.\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Load pre-trained YOLOv8 classification model of chosen size.\n",
    "    model = YOLO(f'yolov8{MODEL_SIZE}-cls')\n",
    "    \n",
    "    # Train model with updated hyperparameters and augmentations.\n",
    "    results = model.train(\n",
    "        data=str(DATASET_ROOT),\n",
    "        epochs=EPOCHS,\n",
    "        batch=BATCH_SIZE,\n",
    "        imgsz=IMG_SIZE,\n",
    "        patience=PATIENCE,\n",
    "        save=True,\n",
    "        save_period=10,\n",
    "        device=device,\n",
    "        workers=4,\n",
    "        project=PROJECT_NAME,\n",
    "        name='train_v7',\n",
    "        exist_ok=True,\n",
    "        pretrained=True,\n",
    "        optimizer='AdamW',\n",
    "        \n",
    "        lr0=LEARNING_RATE,\n",
    "        lrf=0.00001,\n",
    "        momentum=0.937,\n",
    "        weight_decay=0.001,\n",
    "        \n",
    "        warmup_epochs=5,\n",
    "        warmup_momentum=0.8,\n",
    "        warmup_bias_lr=0.01,\n",
    "        cos_lr=True,\n",
    "        \n",
    "        verbose=True,\n",
    "        cache=False,\n",
    "        amp=True,\n",
    "   \n",
    "        hsv_h=0.8,\n",
    "        hsv_s=0.95,\n",
    "        hsv_v=0.95,\n",
    "        \n",
    "        degrees=270.0, # Increased from 180 to 270 for more aggressive rotation.\n",
    "        translate=0.4,\n",
    "        scale=0.9, # Reduced from 0.95 to 0.9\n",
    "        shear=15.0,\n",
    "        perspective=0.002,\n",
    "        flipud=0.5,\n",
    "        fliplr=0.5,\n",
    "        \n",
    "        mosaic=0.0,\n",
    "        mixup=0.5,\n",
    "        copy_paste=0.0,\n",
    "        auto_augment='randaugment',\n",
    "        erasing=0.5, # Reduced from 0.6 to 0.5\n",
    "        crop_fraction=0.55, # Slightly increased from 0.5 to 0.55\n",
    "        \n",
    "        dropout=DROPOUT,\n",
    "        label_smoothing=0.15,\n",
    "        \n",
    "        close_mosaic=15,\n",
    "        plots=True,\n",
    "        val=True,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Entry point.\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model = train_model() # Train the model and store it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9feb1",
   "metadata": {},
   "source": [
    "#### Result (train_v7):\n",
    "\n",
    "- The model successfully handled **busy backgrounds** and **object orientation variations**, which were major challenges in earlier versions.  \n",
    "- Orientation-related misclassifications (e.g., upside-down pens or rotated staplers) were effectively resolved through aggressive rotation augmentations (`degrees=270`).  \n",
    "- The model also became more robust to partial occlusions and cluttered desks thanks to improved cropping and erasing augmentations.  \n",
    "- Visual testing confirmed that predictions remained consistent across lighting and background variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdec0c2",
   "metadata": {},
   "source": [
    "### Validate YOLO Office Supplies Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6513020a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 119\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m acc, macro_f1, cm, class_report\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 54\u001b[0m, in \u001b[0;36mvalidate_model\u001b[1;34m(model_path)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     y_pred\u001b[38;5;241m.\u001b[39mappend(result[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mtop1)\n\u001b[0;32m     56\u001b[0m     y_true\u001b[38;5;241m.\u001b[39mappend(class_idx)\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:187\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    160\u001b[0m     source: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m Path \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    161\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    163\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    164\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:557\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:229\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:38\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 38\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:336\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 336\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    338\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:184\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    179\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    180\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    183\u001b[0m )\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:640\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed, **kwargs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 640\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:139\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:157\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:180\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 180\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    181\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:93\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     84\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m    Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mehar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:543\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    533\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    534\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    542\u001b[0m     )\n\u001b[1;32m--> 543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import all dependencies.\n",
    "from ultralytics import YOLO # YOLO model for object classification.\n",
    "from pathlib import Path # Import pathlib for path manipulation.\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report # Evaluation metrics.\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # To save without gui\n",
    "import matplotlib.pyplot as plt # To create graph.\n",
    "\n",
    "\n",
    "# Function to validate a test dataset and generates metrics and plots.\n",
    "def validate_model(model_path=None):\n",
    "    \n",
    "    # Root folder containing dataset.\n",
    "    DATASET_ROOT = Path(\"data\")  \n",
    "    PROJECT_NAME = \"office_supplies_classifier\"\n",
    "    \n",
    "    # List of class names for classification.\n",
    "    CLASSES = [\n",
    "        \"erasers\", \"glue_sticks\", \"highlighters\", \"mugs\", \"paper_clips\",\n",
    "        \"pencils\", \"pens\", \"staplers\", \"tapes\", \"usb_sticks\"\n",
    "    ]\n",
    "    \n",
    "    # Use default model path if non is provided.\n",
    "    if model_path is None:\n",
    "        model_path = f\"{PROJECT_NAME}/train_v7/weights/best.pt\"\n",
    "    \n",
    "    # Save all reports inside train_v7.\n",
    "    save_dir = Path(PROJECT_NAME) / \"train_v7\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True) # Create diretory if none exist.\n",
    "\n",
    "    # Load the YOLO mode.\n",
    "    model = YOLO(model_path)\n",
    "\n",
    "    # Path to test directory.\n",
    "    test_dir = DATASET_ROOT / \"test\"\n",
    "    if not test_dir.exists():\n",
    "        raise FileNotFoundError(f\"Test directory not found: {test_dir}\")\n",
    "\n",
    "    # Initialise lists for true labels and predicted labels.\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    # Loop through each class folder in test dataset.\n",
    "    for class_idx, class_name in enumerate(CLASSES):\n",
    "        class_path = test_dir / class_name\n",
    "        if not class_path.exists():\n",
    "            print(f\"Skipping missing class folder: {class_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Loop through all image files in the class folder.\n",
    "        for img_path in class_path.glob(\"*.*\"):\n",
    "            \n",
    "            # Only process valid image extensions.\n",
    "            if img_path.suffix.lower() not in {\".jpg\", \".png\", \".jpeg\", \".bmp\"}:\n",
    "                continue\n",
    "            try:\n",
    "                result = model(img_path, verbose=False)\n",
    "                y_pred.append(result[0].probs.top1)\n",
    "                y_true.append(class_idx)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "    # Ensure some test images were processed.\n",
    "    if not y_true:\n",
    "        raise ValueError(\"No test images found!\")\n",
    "\n",
    "    # Convert to numpy arrays for metric calculations.\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    # Compute standard evaluation metrics.\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(len(CLASSES)))\n",
    "    class_report = classification_report(y_true, y_pred, target_names=CLASSES, output_dict=True)\n",
    "\n",
    "    # Extract F1-scores for plotting.\n",
    "    f1_scores = [class_report.get(cls, {}).get(\"f1-score\", 0.0) for cls in CLASSES]\n",
    "\n",
    "    # Plot per-class F1 scores as a line graph.\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    x = range(len(CLASSES))\n",
    "    plt.plot(x, f1_scores, marker=\"o\", linewidth=2, markersize=6, color=\"teal\")\n",
    "    plt.axhline(y=macro_f1, color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Macro F1 = {macro_f1:.3f}\")\n",
    "\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.title(f\"Per-Class F1-Scores (Line Graph)\\nAccuracy: {acc:.1%} | Macro F1: {macro_f1:.3f}\", fontsize=14)\n",
    "    plt.xlabel(\"Class\", fontsize=12)\n",
    "    plt.ylabel(\"F1-Score\", fontsize=12)\n",
    "    plt.xticks(x, CLASSES, rotation=45, ha=\"right\")\n",
    "    plt.grid(True, linestyle=\":\", alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout() \n",
    "\n",
    "    # Plot per-class F1 scores as a line graph\n",
    "    f1_graph_path = save_dir / \"f1_line_graph.png\"\n",
    "    plt.savefig(f1_graph_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save the F1-score graph.\n",
    "    f1_graph_path = save_dir / \"f1_line_graph.png\"\n",
    "    plt.savefig(f1_graph_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save evaluation metrics to a text file.\n",
    "    text_report_path = save_dir / \"metrics.txt\"\n",
    "    with open(text_report_path, \"w\") as f:\n",
    "        f.write(\"OFFICE SUPPLIES CLASSIFIER - ACCURACY REPORT\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        f.write(f\"Model: {model_path}\\n\")\n",
    "        f.write(f\"Accuracy: {acc:.6f} ({acc:.2%})\\n\")\n",
    "        f.write(f\"Macro F1-Score: {macro_f1:.6f}\\n\\n\")\n",
    "\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(\"                 precision    recall  f1-score   support\\n\\n\")\n",
    "        for cls in CLASSES:\n",
    "            metrics = class_report[cls]\n",
    "            f.write(f\"{cls:>15}    {metrics['precision']:.2f}    {metrics['recall']:.2f}    {metrics['f1-score']:.2f}       {int(metrics['support'])}\\n\")\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "        f.write(f\"{'accuracy':>15}                       {acc:.2f}      {int(class_report['accuracy'])}\\n\")\n",
    "        f.write(f\"{'macro avg':>15}    {class_report['macro avg']['precision']:.2f}    {class_report['macro avg']['recall']:.2f}    {class_report['macro avg']['f1-score']:.2f}      {int(class_report['macro avg']['support'])}\\n\")\n",
    "        f.write(f\"{'weighted avg':>15}    {class_report['weighted avg']['precision']:.2f}    {class_report['weighted avg']['recall']:.2f}    {class_report['weighted avg']['f1-score']:.2f}      {int(class_report['weighted avg']['support'])}\\n\")\n",
    "\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    return acc, macro_f1, cm, class_report\n",
    "\n",
    "\n",
    "# Run the validation if script is executed directly.\n",
    "if __name__ == \"__main__\":\n",
    "    validate_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
